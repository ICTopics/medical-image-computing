# Semantic Segmentation

Semantic segmentation is where you separate an image into different regions. This is, in effect, classifying each pixel to what part of the image it belongs. This task can be solved through variations of CNNs. While the CNNs we've seen so far only need to produce an output for an image could be, CNNs to solve semantic segmentation need to produce an output for each pixel. One architecture based on CNNs to solve semantic segmentation is U-Net. This architecture begins the same as a typical CNN, with convolution-activation pairs and max-pooling layers to reduce the image size, while increasing depth. However, after having reduced the image to a small size, there are a series of up-convolutions, which are almost an inverse of the max-pooling layers, as well as convolution-activation pairs which gradually grow the image size, till eventually a full-sized image, representing the segmentation map of the original image is recovered. The series of layers reducing the image size is called the encoder, and the series of layers recovering the image size is called the decoder. U-Net is, therefore, called an encoder-decoder architecture. One important thing about U-Net, that I haven't yet mentioned, is shortcut connections between layers in the encoder to layers in the decoder. These are to assist the decoder in recovering image details. The U-Net architecture was proposed by 3 computer scientists at the University of Freiburg, who used it to win the ISBI cell tracking challenge in 2015.

![](content-images/UNetImage.png)

So what precisely is an up-convolution. The U-Net architecture used 2x2 up-convolutions which went through each pixel in the input image, and uses the entire depth of that pixel to produce 4 output pixels, of depth 1. So each input pixel was converted to 4 output pixels. Multiple up-convolution can be stack to create an output image of higher depth. In U-Net they used half the depth of the previous layer as the number of up-convolutions. So after each up-convolution the depth of the image was half, but the resolution was 4 times greater.

However, an encoder-decoder architecture is not the only solution of semantic segmentation. An encoder-decoder architecture reduces dimension to get a "global view" of the image, before increasing dimension to get back local context. One wonders, whether it is possible for each pixel in the image to get the global context of the image, without reducing the size of the image. This is what was proposed at ICLR (International Conference on Learning Representations) 2016, with dilated convolutions.

![](content-images/DilatedImage.png)